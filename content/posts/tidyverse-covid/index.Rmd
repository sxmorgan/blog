---
title: 'A Simple Tidyverse Workflow'
author: Morgan Essex
date: '2020-05-05'
slug: tidyverse-covid-19
output:
  blogdown::html_page:
    fig_caption: true
    css: tomorrow.css
    toc: true
category: tutorial
tags:
  - tutorial
  - tidyverse
  - r-stats
  - forslund-lab
summary:
    Exploratory data analysis with COVID-19 testing and prognosis in the US.
---

This post is intended to be a follow up resource from my presentation for members of the ECRC. If you haven't seen that or are new here, check out my other [post](../why-tidyverse) on why you should use the tidyverse.

In later posts I will go through a more complete workflow in more fine-grained detail with microbiome-specific examples, but for this introductory demo we are going to be using data from the [COVID Tracking Project](https://covidtracking.com/) in the US. I have little experience teaching so this is an exercise for me as well, and using new data makes that easier to keep what it's like to be a learner in mind.

One of my favorite things about R is the active development community. In March I saw that someone had created a `covid19us` wrapper package to access the COVID Tracking Projects API directly from R; however, at the time of publication, this function sadly [isn't working](https://github.com/aedobbyn/covid19us/issues/17), so we'll use the API.

## Import

Calling `library(tidyverse)` will load all the packages and functions we'll need, including the pipe (`%>%`) operator, but I will explicitly load `magrittr` too so that I can use some aliases, as well as the debatable but economical `%<>%` operator, which saves a variable in place without making you typing the name twice. I'm also going to load the [lubridate](https://lubridate.tidyverse.org/) package for dealing with dates, and I always load the [ggsci](https://cran.r-project.org/web/packages/ggsci/vignettes/ggsci.html) and [ggpubr](https://rpkgs.datanovia.com/ggpubr/index.html) packages for my plots.

```{r knitr, include = FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE)
```

```{r libraries}
# library(covid19us) 
library(tidyverse)
library(magrittr)
library(lubridate)
library(ggsci) 
library(ggpubr)
```

Then, let's load the complete longitudinal dataset, which defaults to a tibble, and select the testing variables that will be relevant for our analysis.

```{r states.data}
states.data <- 'https://covidtracking.com/api/v1/states/daily.csv' %>%
    read_csv() %>%
    select(c('state','date','positive','negative','death','total')) 

states.data
```

```{r states.data.pkg, include = FALSE}
# states.data <- get_states_daily() %>%
#     select(c('state','date','positive','negative','death','total_test_results'))

# states.data
```

Additionally, I have downloaded the (projected) 2020 census estimates of all 50 states + Washington DC and Puerto Rico from https://worldpopulationreview.com, which we'll read in and use to standardize our data to tests per 100,000 residents. 

```{r states.pop}
states.pop <- 'data/covid.us.state.population.csv' %>%
    read_csv() %>%
    select(c('rank','State','Pop')) 

states.pop
```

Lastly, we're going to pull a .csv directly from that website which contains state-abbreviation pairs (e.g. CA, California) that we'll use later to 'beautify' our plots. The 'of' in District of Columbia in this list is capitalized incorrectly, so we'll manually change this using the `str_replace()` function from the [stringr](https://stringr.tidyverse.org/) package to avoid a mismatch later. We'll also add a row for Puerto Rico, since that abbreviation is missing.

```{r states.abbrev}
states.abbrev <- 'https://worldpopulationreview.com/static/states/abbr-name-list.csv' %>%
    read_csv() %>%
    mutate(name = stringr::str_replace(name, 'Of', 'of')) %>%
    add_row(name = 'Puerto Rico', abbreviation = 'PR')

states.abbrev
```

## Tidy and Wrangle

We'll start by joining the three different datasets we have loaded. Our `states.pop` and `states.abbrev` tibbles are both tidy and consist of 52 data points, but our `states.data` is longitudinal, so we have a lot more than 52 data points. So starting our pipe with that and using `left_join()` in succession ensures the population and abbreviation are duplicated for each date in our final joined tibble.

```{r raw.joined}
raw.testing.data <- states.data %>%
    left_join(states.abbrev, by = c('state' = 'abbreviation')) %>%
    left_join(states.pop, by = c('name' = 'State')) %>%
    rename(state.name = 'name') %>%
    filter(!is.na(state.name))

# peeking at newly joined variables
raw.testing.data %>%
    select(c('state','date','state.name','Pop')) %>%
    arrange(state.name)
```

Our dates are in an ugly format, so we'll use the `ymd()` command in the `lubridate` package to quickly parse it into a 'year-month-day' (ymd) format. To cut down on the size of our data of interest, let's only look at dates after March 9th, just before many lockdown periods began.

```{r date.filtered}
testing.data <- raw.testing.data %>%
    mutate_at(vars(date), ~ as_date(ymd(.))) %>%
    filter(date > '2020-03-09')

# check trimmed date range
testing.data %>%
    use_series(date) %>%
    range()
```

Now that we have our dates sorted out, we can standardize our testing counts per 100k residents. To do this, we'll use `mutate()` to create a column called `pop.factor` which divides the population by 100,000 to get the number of "groups" of 100k residents in that population. We'll `mutate()` each of our count columns, dividing by our `pop.factor`, to get our standardized values. We will select only these new columns and the other variables that we'll need to create some basic longitudinal scatter plots, ignoring the intermediate variables like `pop.factor`.

```{r standardized.trimmed}
testing.data %<>%
    mutate(pop.factor = Pop/100000) %>%
    mutate(tests.std = total/pop.factor) %>%
    mutate(deaths.std = death/pop.factor) %>%
    mutate(positive.std = positive/pop.factor) %>%
    mutate(negative.std = negative/pop.factor) %>%
    select(c('state.name','date','tests.std','deaths.std','positive.std','negative.std'))

testing.data
```

By default, our states are sorted alphabetically, but what if we wanted to sort them according to their testing rates?

In a quick console sidebar, let's see which states have the highest testing rates.

```{r sidebar.arrange}
testing.data %>%
    arrange(desc(tests.std))
```

In order to get a better overview, we can group our row observations by state and then nest the remaining data, which will collapse the dates and leave us with a `state.name` column where we can quickly see the ten states with the highest testing per 100k residents.

```{r sidebar.nest}
testing.data %>%
    arrange(desc(tests.std)) %>%
    group_by(state.name) %>%
    nest()
```

## Visualize

Our `testing.data` is now nice and tidy, but if you're familiar with `ggplot2` then you proabably already know that our data is not quite "plot tidy." This is not an official term, but I find it useful because it reminds me that data almost always needs to be additionally manipulated in specific ways to get a final plot. 

Right now it is difficult to plot our four different testing variables on the same plot, since they're in four different columns, which would require four different `geom_point()` commands (each with a different y-axis variable). In order for us to have "plot tidy" data, we want to collapse these four columns into two 'key-value' pair columns instead: `std.metric` and `std.value`.

For this the `dplyr::pivot_longer()` function [formerly `gather()`](https://github.com/tidyverse/tidyr/releases/tag/v1.0.0) will be essential. We'll tell it to ignore the state and date columns using the `cols` parameter. 

```{r plot.tidy}
testing.data %>%
    pivot_longer(cols = -c('date','state.name'),
                 names_to = 'std.metric',
                 values_to = 'std.value')
```

Perfect. But if we want to sort our states by `tests.std` like we did above, we need to do that _before_ we collapse that variable to get everything plot tidy, hence why we didn't use `%<>%` above to save our new tibble. 

We'll eventually use the `facet_wrap()` command to plot each state separately in a grid, and if we want those to be in the same order as our states, we have to make our `state.name` variable an ordered factor. Just like console sidebar above we'll first group our tibble by state and then nest and ungroup it, since nesting structurally groups the data for us. By using `mutate()` and populating our new `ordered` column from `1:n()`, each state has a unique rank.

```{r}
plot.data <- testing.data %>%
    arrange(desc(tests.std)) %>%
    group_by(state.name) %>%
    nest() %>%
    ungroup() %>%
    mutate(ordered = 1:n())

plot.data
```

A side note, once I start making plot tidy manipulations I create a new `plot.data` tibble. That way, I have my `testing.data` tibble available for console sidebars to help me refine the information in `plot.data` according to what I want `ggplot2` to use and display. 

Now that we have sorted the data and assigned each state a rank, we want to use `fct_reorder()` to reorder `state.name` by `ordered` to complete the transformation, and then unnest our tibble so we can keep going.

```{r}
plot.data %<>%
    unnest(data) %>%
    mutate_at(vars(state.name), ~ fct_reorder(., ordered)) %>%
    select(-ordered)

plot.data
```

Now that we've taken care of ordering our states, we have to collapse our four testing variables and save it this time. We'll use the same `pivot_longer()` command we did above, except on `plot.data` this time and with `%<>%`.

```{r}
plot.data %<>%
    pivot_longer(cols = -c('date','state.name'),
                 names_to = 'std.metric',
                 values_to = 'std.value')

plot.data
```

Finally, we'll beautify `plot.data` by reordering our `std.metric` variable to get the shapes and colors we want, and changing the names to be a bit more intuitive for the final plots.

```{r plot.beauty}
plot.data %<>%
    mutate_at(vars(std.metric), ~ fct_relevel(., 'tests.std','negative.std',
                                              'positive.std','deaths.std')) %>%
    mutate_at(vars(std.metric), ~ fct_recode(., 
                                              `Total Test Results` = 'tests.std',
                                             `Negative Tests` = 'negative.std',
                                              `Positive Tests` = 'positive.std',
                                              `Deaths` = 'deaths.std')) 
    
plot.data
```

We're finally ready to plot! 

We'll use `geom_point()` so that each `std.value` is plotted individually, and use our ordered `std.metric` factor created above to dictate the color and shape. Because we used the `lubridate` package earlier to parse our dates, you might have noticed from the tibble outputs that our date column became a 'date' data type. `ggplot2` has nice built-in functions for date scales, so we'll use `scale_x_date()` and tell it that we want to tick every third week and display the dates in 'dd.mm' format. 

If you haven't already I encourage you to explore the `ggsci` and `ggpubr` packages which offer a lot of functionality and style options. My go-to is the `scale_color_futurama()` color palette and the `theme_pubclean()` theme which really highlights the data trend of interest in my opinion, and also moves the legend to the top of the plot.

```{r, fig.height = 20}
testing.plot <- plot.data %>%
    ggplot(aes(x = date, y = std.value)) +
    geom_point(aes(color = std.metric, shape = std.metric)) +
    scale_x_date(date_breaks = '3 weeks',
                 date_labels = '%d.%m') +
    scale_color_futurama() +
    facet_wrap(~ state.name, ncol = 4) +
    # fix title, axes, legends
    labs(title = 'COVID-19 Testing and Prognosis in US States',
         x = '', y = '', color = 'Per 100k Residents:', shape = 'Per 100k Residents:') +
    theme_pubclean()

testing.plot
```

## Summarize 

There you have it. 

```{css, echo = FALSE}
img {
    display: block;
    margin-left: auto;
    margin-right: auto;
    width: 70%;
    padding: 10px
}
```