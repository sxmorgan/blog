---
title: 'A Simple Tidyverse Workflow'
author: Morgan Essex
date: '2020-05-05'
slug: tidyverse-covid-19
output:
  blogdown::html_page:
    fig_caption: true
    css: tomorrow.css
    toc: true
category: tutorial
tags:
  - tutorial
  - tidyverse
  - r-stats
  - forslund-lab
summary:
    Exploratory data analysis with COVID-19 testing and prognosis in the US.
---

This post is intended to be a follow up resource from my presentation for members of the ECRC. If you haven't seen that or are new here, check out my other [post](../why-tidyverse) on why you should use the tidyverse and get familiar with the syntax and functions I'll be using, and find the raw R script used to generate this tutorial in my [ecrc-data-science](https://github.com/sxmorgan/ecrc-data-science) repository on GitHub. 

In later posts I will go through a more complete workflow in more fine-grained detail with microbiome-specific examples, but for this introductory demo we are going to be using data from the [COVID Tracking Project](https://covidtracking.com/) in the US. I have basically no experience teaching so this is an exercise for me too, and data that's also unfamiliar to me makes it easier to keep in mind what it's like to be a learner.

One of my favorite things about R is the active development community. In March I saw that someone had created a `covid19us` wrapper package to access the COVID Tracking Project's API directly from R; however, at the time of writing, this function sadly [isn't working](https://github.com/aedobbyn/covid19us/issues/17), so we'll use the API.

## Import

Calling `library(tidyverse)` will load all the packages and functions we'll need, including the pipe (`%>%`) operator. I will explicitly load `magrittr` too so that I can use some aliases, as well as the debatable but economical `%<>%` operator, which saves a variable in place. I'm also going to load the [lubridate](https://lubridate.tidyverse.org/) package for dealing with dates, and I always load the [ggsci](https://cran.r-project.org/web/packages/ggsci/vignettes/ggsci.html) and [ggpubr](https://rpkgs.datanovia.com/ggpubr/index.html) packages for my plots.

```{r knitr, include = FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE)
```

```{r libraries}
# library(covid19us) ;(
library(tidyverse)
library(magrittr)
library(lubridate)
library(ggsci) 
library(ggpubr)
```

Then, let's call `read_csv()` on the complete longitudinal dataset url, which defaults to a tibble, and `select()` the testing counts that will be relevant for our analysis: positive, negative, death, and total.

```{r states.data}
states.data <- 'https://covidtracking.com/api/v1/states/daily.csv' %>%
    read_csv() %>%
    select(c('state','date','positive','negative','death','total')) 

states.data
```

```{r states.data.pkg, include = FALSE}
# states.data <- get_states_daily() %>%
#     select(c('state','date','positive','negative','death','total_test_results'))

# states.data
```

Additionally, I have downloaded the (projected) 2020 census estimates of all 50 states + Washington DC and Puerto Rico from https://worldpopulationreview.com, which we'll read in and use to standardize our tests per 100,000 residents. 

```{r states.pop}
states.pop <- 'data/covid.us.state.population.csv' %>%
    read_csv() %>%
    select(c('rank','State','Pop')) 

states.pop
```

Lastly, we're going to pull another .csv directly from that website which contains state-abbreviation pairs (e.g. CA, California) that we'll use later to "beautify" our plots. The 'of' in District of Columbia in this list is capitalized incorrectly, so we'll manually `mutate()` this and apply the `str_replace()` function from the [stringr](https://stringr.tidyverse.org/) package to avoid a mismatch later. We'll also add a row for Puerto Rico, since that abbreviation is missing.

```{r states.abbrev}
states.abbrev <- 'https://worldpopulationreview.com/static/states/abbr-name-list.csv' %>%
    read_csv() %>%
    mutate(name = stringr::str_replace(name, 'Of', 'of')) %>%
    add_row(name = 'Puerto Rico', abbreviation = 'PR')

states.abbrev
```

## Tidy and Wrangle

We'll start by joining the three different datasets we have loaded. Our `states.pop` and `states.abbrev` tibbles are both tidy and consist of 52 data points, but our `states.data` is longitudinal, so we have a lot more than 52 data points. It makes the most sense to start our pipe with that and use `left_join()` in succession to ensure that the population and abbreviation are duplicated for each date in our final joined tibble. 

The COVID Tracking Project has data on 56 states and territories in total, but since we'd have to go look up the populations and full names of those additional four territories, we will ignore them for now. We'll `rename()` our column with the full state names as `state.name`, and then filter out states for which we didn't have a full name, i.e. the extra four territories.

```{r raw.joined}
raw.testing.data <- states.data %>%
    left_join(states.abbrev, by = c('state' = 'abbreviation')) %>%
    left_join(states.pop, by = c('name' = 'State')) %>%
    rename(state.name = 'name') %>%
    filter(!is.na(state.name))

# peeking at newly joined tbl
raw.testing.data %>%
    select(c('state','date','state.name','Pop')) %>%
    arrange(state.name)
```

Our dates are in an ugly format, so we'll use the `ymd()` command in the `lubridate` package to quickly parse it into a prettier 'year-month-day' format. To cut down on the size of our data of interest, let's only look at dates after March 9th, just before many lockdown periods began.

```{r date.filtered}
testing.data <- raw.testing.data %>%
    mutate_at(vars(date), ~ as_date(ymd(.))) %>%
    filter(date > '2020-03-09')

# check trimmed date range
testing.data %>%
    use_series(date) %>%
    range()
```

I know the syntax of the mutate command above probably looks super weird. Firstly, I've opted for `mutate_at()` which doesn't assign new values using an `=` sign like the standard `mutate()` does, but rather a `vars()` parameter which applies whatever function is specified as the second argument to the raw data in `vars()`. The function could be something simple, like `sum()`, an anonymous function similar to the `apply()` syntax (`function(x) {do something with x}`), or a shorthand as I've done.

The shorthand is part of the `purrr` package I will dedicate a specific presentation/tutorial to because it's a  clever and elegant way to solve **_so_** many problems. For now, just know that since I used `mutate_at()` and `vars()` and got a vector, I can use a shorthand where the `~` allows me to feed each element of the first argument to the desired functions in the second argument using `.`.

Essentially in one line we're saying "take the dates which were incorrectly read in as doubles _(the first argument from `vars()`)_, parse as ymd format with `ymd(.)`, and return formatted date objects in their place." Think of `.` as a pronoun referring to the current element being operated on, like the `i` iterator in for-loops.

People who dislike `apply` functions for their lack of readability probably won't like the shorthand, but I'll readily admit that I use it whenever I can because a) I prefer concise code and b) it's an easy way to practice something that opens up tons of really advanced avenues. Using it has definitely made me a better programmer because I've been forced to think about _functions_. `purrr` is more consistent than the `apply()` family of functions, so it's easier to learn as well.

Here's another way that could be done without the shorthand notation, similar to `apply()`.

```{r date.sidebar}
testing.data <- raw.testing.data %>%
    mutate_at(vars(date), function(x) {
        x %>%
            ymd() %>%
            as_date() }) %>%
    filter(date > '2020-03-09')

# check trimmed date range
testing.data %>%
    use_series(date) %>%
    range()
```

And lastly, here's how that would look if we used the standard `mutate()` command. Clearly I started here and played around _a lot_ to get to where the shorthand is natural. This code in two steps is more intuitively readable but less concise.

```{r date.sidebar.mutate}
testing.data <- raw.testing.data %>%
    mutate(date = ymd(date)) %>%
    mutate(date = as_date(date)) %>%
    filter(date > '2020-03-09')

# check trimmed date range
testing.data %>%
    use_series(date) %>%
    range()
```

Now that we have our dates sorted out, we can standardize our testing counts per 100k residents. To do this, we'll create a column called `pop.factor` which divides the population by 100,000 to get the number of "groups" of 100k residents in that population. We'll `mutate()` each of our count columns, dividing by our `pop.factor`, to get our standardized values. We will select only these new columns and the other variables that we'll need to create some basic longitudinal scatter plots, ignoring the intermediate variables like `pop.factor`.

```{r standardized.trimmed}
testing.data %<>%
    mutate(pop.factor = Pop/100000) %>%
    mutate(tests.std = total/pop.factor) %>%
    mutate(deaths.std = death/pop.factor) %>%
    mutate(positive.std = positive/pop.factor) %>%
    mutate(negative.std = negative/pop.factor) %>%
    select(c('state.name','date','tests.std','deaths.std','positive.std','negative.std'))

testing.data
```

By default, our states are sorted alphabetically, but what if we wanted to sort them according to their testing coverage?

In a quick console sidebar, let's see which states are performing the most testing.

```{r sidebar.arrange}
testing.data %>%
    arrange(desc(tests.std))
```

In order to get a better overview, we can group our row observations by state and then nest the remaining data, which will collapse everything and leave us with a `state.name` column where we can quickly see the ten states with the highest testing per 100k residents.

```{r sidebar.nest}
temp <- testing.data %>%
    arrange(desc(tests.std)) %>%
    group_by(state.name) %>%
    nest()

temp
```

The first item in our nested list-column contains a tibble for Rhode Island's data (shown below), the second item a tibble for New York, etc.  

```{r sidebar.peek}
temp$data[[1]]
```

## Visualize

Our `testing.data` is now nice and tidy, but if you're familiar with `ggplot2` then you proabably already know that our data is not quite "plot tidy." This is not an official term, but I find it useful because getting a decent plot almost always requires additional (and data-specific) manipulations. 

Right now we have four different testing variables for each state, since they're in four different columns, which would translate into four different `geom_point()` commands (each with a different y-axis variable) in `ggplot2`. In order for us to have "plot tidy" data, we want to collapse these four columns into two 'key-value' pair columns instead: `std.metric` and `std.value`.

For this the `dplyr::pivot_longer()` function, [formerly known as](https://github.com/tidyverse/tidyr/releases/tag/v1.0.0) `gather()` will be essential. We'll tell it to ignore the state and date columns by using the `cols` parameter. 

```{r plot.tidy}
testing.data %>%
    pivot_longer(cols = -c('date','state.name'),
                 names_to = 'std.metric',
                 values_to = 'std.value')
```

Perfect. But if we want to sort our states by `tests.std` like we did above, we need to do that _before_ we collapse that variable to get everything plot tidy, hence why we didn't use `%<>%` above to save our new tibble. 

A side note, once I start making plot tidy manipulations I create a new `plot.data` tibble. That way, I have my `testing.data` tibble available for console sidebars to help me refine the information in `plot.data` according to what I want `ggplot2` to use and display. I've played with this enough to know that in advance so that this tutorial goes a little smoother, but it's just a suggestion/tip that's helped me.

Eventually we will use the `facet_wrap()` command to plot each state separately in a grid. Just as a few code chunks ago, this will be alphabetical by default. If we rather want the grid to display states in descending order by their `tests.std` values, we have to make our `state.name` variable an _ordered_ factor, which requires a few steps that probably seem a little convuluted, but are the sort of little things that become second nature over time. 

Just like console sidebar above we'll first group our tibble by state and then nest and ungroup it, since nesting "cements" and structurally groups the data for us, so we don't need it anymore and it can cause problems later. By using `mutate()` and populating our new `ordered` column from `1:n()`, each state has a unique rank.

```{r}
plot.data <- testing.data %>%
    arrange(desc(tests.std)) %>%
    group_by(state.name) %>%
    nest() %>%
    ungroup() %>%
    mutate(ordered = 1:n())

plot.data
```

The last step to prepare our grid order now that we have sorted the data and assigned each state a rank is to use `fct_reorder()` to mutate `state.name` by `ordered`. Then we can unnest our tibble in place with the `%<>%` operator and keep going. We don't need `ordered` any more so we can deselect it.

```{r}
plot.data %<>%
    unnest(data) %>%
    mutate_at(vars(state.name), ~ fct_reorder(., ordered)) %>%
    select(-ordered)

plot.data
```

Now that we've taken care of ordering, we have to collapse our four testing variables and save it this time. We'll use the same `pivot_longer()` command we did above.

```{r}
plot.data %<>%
    pivot_longer(cols = -c('date','state.name'),
                 names_to = 'std.metric',
                 values_to = 'std.value')

plot.data
```

Finally, we'll beautify `plot.data` by reordering our `std.metric` variable to get the shapes and colors we want (I have prefigured to save time), and changing the names to be a bit more intuitive for the final plots.

```{r plot.beauty}
plot.data %<>%
    mutate_at(vars(std.metric), ~ fct_relevel(., 'tests.std','negative.std',
                                              'positive.std','deaths.std')) %>%
    mutate_at(vars(std.metric), ~ fct_recode(., 
                                              `Total Test Results` = 'tests.std',
                                             `Negative Tests` = 'negative.std',
                                              `Positive Tests` = 'positive.std',
                                              `Deaths` = 'deaths.std')) 
    
plot.data
```

We're finally ready to plot! 

We'll use `geom_point()` so that each `std.value` is plotted individually, and use our ordered `std.metric` factor created above to dictate the color and shape. Because we used the `lubridate` package earlier to parse our dates, you might have noticed from the tibble outputs that our date column became a <date> data type. `ggplot2` has nice built-in functions for date scales, so we'll use `scale_x_date()` and tell it that we want to tick every third week and display the dates in 'dd.mm' format. 

If you haven't already I encourage you to explore the `ggsci` and `ggpubr` packages which offer a lot of functionality and style options. My go-to is the `scale_color_futurama()` color palette and the `theme_pubclean()` theme from each, respectively. The theme is a minimal one that helps highlight the data trend of interest and moves the legend to the top of the plot.

```{r, fig.height = 20}
testing.plot <- plot.data %>%
    ggplot(aes(x = date, y = std.value)) +
    geom_point(aes(color = std.metric, shape = std.metric)) +
    scale_x_date(date_breaks = '3 weeks',
                 date_labels = '%d.%m') +
    scale_color_futurama() +
    facet_wrap(~ state.name, ncol = 4) +
    # fix title, axes, legends
    labs(title = 'COVID-19 Testing and Prognosis in US States',
         x = '', y = '', color = 'Per 100k Residents:', shape = 'Per 100k Residents:') +
    theme_pubclean()

testing.plot
```




```{css, echo = FALSE}
img {
    display: block;
    margin-left: auto;
    margin-right: auto;
    width: 70%;
    padding: 10px
}
```